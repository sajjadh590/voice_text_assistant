#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                           OMNI-HEAR AI v2.4                                  ‚ïë
‚ïë              Fixed Model Names + Better Error Handling                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  ‚úÖ CORRECT MODEL NAMES: Using latest model identifiers                      ‚ïë
‚ïë  üìù BETTER LOGGING: Shows exact error messages                               ‚ïë
‚ïë  üîÑ SMART RETRY: Waits and retries on rate limit                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import os
import sys
import logging
import base64
import asyncio
import time
import traceback
from typing import Optional, List, Tuple

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters,
)

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

# ============== LOGGING ==============
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ============== CONFIGURATION ==============
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ============== VALIDATE API KEY ==============
if not GEMINI_API_KEY:
    logger.error("‚ùå GEMINI_API_KEY is not set!")
    print("‚ö†Ô∏è  Please set GEMINI_API_KEY environment variable")
else:
    logger.info(f"‚úÖ GEMINI_API_KEY configured (length: {len(GEMINI_API_KEY)})")
    genai.configure(api_key=GEMINI_API_KEY)

# ============== CORRECT MODEL NAMES ==============
# Updated to use correct model identifiers that actually work!
MODEL_PRIORITY: List[str] = [
"gemini-2.5-flash-lite",      # ü•á ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ≥ŸáŸÖ€åŸá ÿ±ÿß€å⁄ØÿßŸÜ (1,000 ÿ™ÿß 1,500 ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ÿØÿ± ÿ±Ÿàÿ≤)
    "gemini-2.0-flash",           # ü•à ŸÖÿØŸÑ ÿßÿ≥ÿ™ÿßŸÜÿØÿßÿ±ÿØ (1,000 ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ÿØÿ± ÿ±Ÿàÿ≤)
    "gemini-2.5-flash",           # ü•â ÿ≥ŸáŸÖ€åŸá ŸÖÿ≠ÿØŸàÿØ ÿ¥ÿØŸá (ŸÅŸÇÿ∑ €≤€∞ ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ÿØÿ± ÿ±Ÿàÿ≤)
    "gemini-1.5-flash-latest",    # üõ°Ô∏è ÿ≤ÿßŸæÿßÿ≥ ŸÜŸáÿß€å€å (ŸÖÿØŸÑ ŸÇÿØ€åŸÖ€å)
]

MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB

# ============== SYSTEM PROMPTS ==============
PROMPTS = {
    "lecture": """You are a University Professor teaching in Persian (Farsi).
Listen to this audio carefully. Do NOT summarize.
Write a comprehensive **Textbook Chapter in Persian**.
Cover every single detail, example, and nuance mentioned.
Use bold headers (ÿ®ÿß ** ÿπŸÑÿßŸÖÿ™‚Äå⁄Øÿ∞ÿßÿ±€å ⁄©ŸÜ€åÿØ) to organize sections.
The goal is to replace the need to listen to the audio entirely.
Write in fluent, academic Persian. ÿ≤ÿ®ÿßŸÜ ÿÆÿ±Ÿàÿ¨€å ÿ≠ÿ™ŸÖÿßŸã ŸÅÿßÿ±ÿ≥€å ÿ®ÿßÿ¥ÿØ.""",

    "soap": """You are a Chief Resident at a teaching hospital.
Listen to this medical dictation audio.
Write a professional **SOAP Note in English**.
Format:
**Subjective:** (Chief complaint, HPI, ROS, PMH, medications, allergies)
**Objective:** (Vitals, physical exam findings, lab results, imaging)
**Assessment:** (Diagnoses with ICD codes if possible)
**Plan:** (Treatment plan, medications, follow-up)
Correct all medical terminology. Output MUST be in English only.""",

    "summary": """Listen to this audio carefully.
Summarize the content into clear, concise **Persian bullet points**.
Use ‚Ä¢ for bullet points. Write in fluent Persian.
Focus on the most important information.
ÿ≤ÿ®ÿßŸÜ ÿÆÿ±Ÿàÿ¨€å ÿ≠ÿ™ŸÖÿßŸã ŸÅÿßÿ±ÿ≥€å ÿ®ÿßÿ¥ÿØ.""",

    "lyrics": """Listen to this audio.
If it contains music: Extract and provide the complete lyrics in the original language.
If it contains speech: Provide a verbatim transcription in the original language.
Format the output cleanly with proper line breaks."""
}

# Persian messages
MESSAGES = {
    "welcome": """üéß **ÿ®Ÿá Omni-Hear AI ÿÆŸàÿ¥ ÿ¢ŸÖÿØ€åÿØ!**

üé§ €å⁄© ŸÅÿß€åŸÑ ÿµŸàÿ™€å €åÿß Ÿà€åÿ≥ ÿßÿ±ÿ≥ÿßŸÑ ⁄©ŸÜ€åÿØ.

‚ö° ŸÇÿßÿ®ŸÑ€åÿ™‚ÄåŸáÿß:
‚Ä¢ üìö ÿØÿ±ÿ≥ŸÜÿßŸÖŸá ⁄©ÿßŸÖŸÑ (ŸÅÿßÿ±ÿ≥€å)
‚Ä¢ ü©∫ ÿ¥ÿ±ÿ≠‚Äåÿ≠ÿßŸÑ Ÿæÿ≤ÿ¥⁄©€å SOAP (ÿßŸÜ⁄ØŸÑ€åÿ≥€å)
‚Ä¢ üìù ÿÆŸÑÿßÿµŸá ŸÖÿ™ŸÜ (ŸÅÿßÿ±ÿ≥€å)
‚Ä¢ üéµ ŸÖÿ™ŸÜ ÿ¢ŸáŸÜ⁄Ø

üîÑ ŸÜÿ≥ÿÆŸá 2.4 - Ÿæÿß€åÿØÿßÿ± Ÿà ÿ≥ÿ±€åÿπ""",
    "audio_received": "üéµ ŸÅÿß€åŸÑ ÿØÿ±€åÿßŸÅÿ™ ÿ¥ÿØ!\n\nüìã ŸÜŸàÿπ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ±ÿß ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ŸÜ€åÿØ:",
    "processing": "‚è≥ ÿØÿ± ÿ≠ÿßŸÑ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ®ÿß ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å...\n\n‚è± ŸÑÿ∑ŸÅÿßŸã ÿµÿ®ÿ± ⁄©ŸÜ€åÿØ (€±€∞-€≥€∞ ÿ´ÿßŸÜ€åŸá)",
    "error": "‚ùå ÿÆÿ∑ÿß ÿØÿ± Ÿæÿ±ÿØÿßÿ≤ÿ¥. ŸÑÿ∑ŸÅÿßŸã ÿØŸàÿ®ÿßÿ±Ÿá ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åÿØ.",
    "quota_exceeded": "‚ö†Ô∏è ÿ≥ŸÇŸÅ ÿßÿ≥ÿ™ŸÅÿßÿØŸá API ÿ™ŸÖÿßŸÖ ÿ¥ÿØŸá.\n\nüí° ŸÑÿ∑ŸÅÿßŸã ⁄ÜŸÜÿØ ÿØŸÇ€åŸÇŸá ÿµÿ®ÿ± ⁄©ŸÜ€åÿØ €åÿß ÿ®ÿß ÿßÿØŸÖ€åŸÜ ÿ™ŸÖÿßÿ≥ ÿ®⁄Ø€åÿ±€åÿØ.",
    "all_failed": "‚ùå ÿÆÿ∑ÿß: {details}\n\nüîÑ ŸÑÿ∑ŸÅÿßŸã ÿØŸàÿ®ÿßÿ±Ÿá ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åÿØ.",
    "no_audio": "‚ö†Ô∏è ŸÑÿ∑ŸÅÿßŸã ÿßÿ®ÿ™ÿØÿß €å⁄© ŸÅÿß€åŸÑ ÿµŸàÿ™€å ÿßÿ±ÿ≥ÿßŸÑ ⁄©ŸÜ€åÿØ.",
    "file_too_large": "‚ö†Ô∏è ÿ≠ÿ¨ŸÖ ŸÅÿß€åŸÑ ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≤ €≤€∞ ŸÖ⁄Øÿßÿ®ÿß€åÿ™ ÿßÿ≥ÿ™.",
    "not_audio": "‚ö†Ô∏è ŸÑÿ∑ŸÅÿßŸã €å⁄© ŸÅÿß€åŸÑ ÿµŸàÿ™€å ÿßÿ±ÿ≥ÿßŸÑ ⁄©ŸÜ€åÿØ.",
    "api_key_missing": "‚ö†Ô∏è ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ÿ≥ÿ±Ÿàÿ± ŸÜÿßŸÇÿµ ÿßÿ≥ÿ™. GEMINI_API_KEY ÿ™ŸÜÿ∏€åŸÖ ŸÜÿ¥ÿØŸá!",
}

# Store user audio files temporarily
user_audio_cache: dict = {}


def get_menu_keyboard() -> InlineKeyboardMarkup:
    """Create the Persian inline keyboard menu."""
    return InlineKeyboardMarkup([
        [
            InlineKeyboardButton("üìö ÿØÿ±ÿ≥ŸÜÿßŸÖŸá ⁄©ÿßŸÖŸÑ", callback_data="lecture"),
            InlineKeyboardButton("ü©∫ ÿ¥ÿ±ÿ≠‚Äåÿ≠ÿßŸÑ Ÿæÿ≤ÿ¥⁄©€å", callback_data="soap"),
        ],
        [
            InlineKeyboardButton("üìù ÿÆŸÑÿßÿµŸá ŸÖÿ™ŸÜ", callback_data="summary"),
            InlineKeyboardButton("üéµ ŸÖÿ™ŸÜ ÿ¢ŸáŸÜ⁄Ø", callback_data="lyrics"),
        ],
    ])


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle the /start command."""
    await update.message.reply_text(MESSAGES["welcome"], parse_mode="Markdown")


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle the /help command."""
    help_text = """üìñ **ÿ±ÿßŸáŸÜŸÖÿß€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá**

1Ô∏è‚É£ €å⁄© ŸÅÿß€åŸÑ ÿµŸàÿ™€å €åÿß Ÿà€åÿ≥ ÿßÿ±ÿ≥ÿßŸÑ ⁄©ŸÜ€åÿØ
2Ô∏è‚É£ ÿßÿ≤ ŸÖŸÜŸà ŸÜŸàÿπ Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ±ÿß ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ŸÜ€åÿØ
3Ô∏è‚É£ ŸÖŸÜÿ™ÿ∏ÿ± ŸÜÿ™€åÿ¨Ÿá ÿ®ŸÖÿßŸÜ€åÿØ (€±€∞-€≥€∞ ÿ´ÿßŸÜ€åŸá)

**ÿ≠ÿßŸÑÿ™‚ÄåŸáÿß€å Ÿæÿ±ÿØÿßÿ≤ÿ¥:**
üìö **ÿØÿ±ÿ≥ŸÜÿßŸÖŸá ⁄©ÿßŸÖŸÑ** - ŸÖÿ™ŸÜ ÿØÿ±ÿ≥€å ⁄©ÿßŸÖŸÑ ÿ®Ÿá ŸÅÿßÿ±ÿ≥€å
ü©∫ **ÿ¥ÿ±ÿ≠‚Äåÿ≠ÿßŸÑ Ÿæÿ≤ÿ¥⁄©€å** - SOAP Note ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å
üìù **ÿÆŸÑÿßÿµŸá ŸÖÿ™ŸÜ** - ÿÆŸÑÿßÿµŸá ŸÜ⁄©ÿßÿ™ ÿ®Ÿá ŸÅÿßÿ±ÿ≥€å
üéµ **ŸÖÿ™ŸÜ ÿ¢ŸáŸÜ⁄Ø** - ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÖÿ™ŸÜ/ŸÑ€åÿ±€å⁄©

üí° ÿ≠ÿØÿß⁄©ÿ´ÿ± ÿ≠ÿ¨ŸÖ: €≤€∞ ŸÖ⁄Øÿßÿ®ÿß€åÿ™
ü§ñ ŸÖÿØŸÑ: Gemini 2.0"""
    await update.message.reply_text(help_text, parse_mode="Markdown")


async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Check bot status and API connectivity."""
    status_parts = ["üîç **Ÿàÿ∂ÿπ€åÿ™ ÿ≥€åÿ≥ÿ™ŸÖ:**\n"]
    
    # Check Telegram Token
    if TELEGRAM_BOT_TOKEN:
        status_parts.append("‚úÖ Telegram Token: ŸÅÿπÿßŸÑ")
    else:
        status_parts.append("‚ùå Telegram Token: ÿ™ŸÜÿ∏€åŸÖ ŸÜÿ¥ÿØŸá!")
    
    # Check Gemini API Key
    if GEMINI_API_KEY:
        status_parts.append(f"‚úÖ Gemini API Key: ÿ™ŸÜÿ∏€åŸÖ ÿ¥ÿØŸá")
        
        # Test Gemini connection with a simple request
        try:
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = await asyncio.to_thread(
                model.generate_content,
                "Say 'API Working' in exactly 2 words."
            )
            if response.text:
                status_parts.append("‚úÖ Gemini API: ŸÖÿ™ÿµŸÑ Ÿà ŸÅÿπÿßŸÑ ‚ú®")
                status_parts.append(f"   Ÿæÿßÿ≥ÿÆ ÿ™ÿ≥ÿ™: {response.text[:50]}")
        except google_exceptions.ResourceExhausted:
            status_parts.append("‚ö†Ô∏è Gemini API: Quota ÿ™ŸÖÿßŸÖ ÿ¥ÿØŸá!")
            status_parts.append("   üí° ŸÜ€åÿßÿ≤ ÿ®Ÿá API Key ÿ¨ÿØ€åÿØ ÿØÿßÿ±€åÿØ")
        except google_exceptions.InvalidArgument as e:
            status_parts.append(f"‚ùå Gemini API: ÿÆÿ∑ÿß€å Ÿæÿßÿ±ÿßŸÖÿ™ÿ±")
        except Exception as e:
            status_parts.append(f"‚ùå Gemini API Error: {str(e)[:80]}")
    else:
        status_parts.append("‚ùå Gemini API Key: ÿ™ŸÜÿ∏€åŸÖ ŸÜÿ¥ÿØŸá!")
    
    status_parts.append(f"\nüîÑ ŸÖÿØŸÑ‚ÄåŸáÿß:\n   " + "\n   ".join(MODEL_PRIORITY))
    
    await update.message.reply_text("\n".join(status_parts), parse_mode="Markdown")


async def models_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """List available models."""
    try:
        models_list = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                models_list.append(f"‚Ä¢ `{m.name.replace('models/', '')}`")
        
        if models_list:
            text = "ü§ñ **ŸÖÿØŸÑ‚ÄåŸáÿß€å ŸÖŸàÿ¨ŸàÿØ:**\n\n" + "\n".join(models_list[:15])
            if len(models_list) > 15:
                text += f"\n\n... Ÿà {len(models_list) - 15} ŸÖÿØŸÑ ÿØ€å⁄Øÿ±"
        else:
            text = "‚ùå Ÿá€å⁄Ü ŸÖÿØŸÑ€å €åÿßŸÅÿ™ ŸÜÿ¥ÿØ!"
            
        await update.message.reply_text(text, parse_mode="Markdown")
    except Exception as e:
        await update.message.reply_text(f"‚ùå ÿÆÿ∑ÿß: {str(e)[:100]}")


async def handle_audio(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle incoming audio files and voice messages."""
    user_id = update.effective_user.id
    msg = update.message
    
    # Check API Key first
    if not GEMINI_API_KEY:
        await msg.reply_text(MESSAGES["api_key_missing"])
        return
    
    audio_file = None
    file_type = "audio"
    
    if msg.voice:
        audio_file = msg.voice
        file_type = "voice"
    elif msg.audio:
        audio_file = msg.audio
    elif msg.document:
        if msg.document.mime_type and msg.document.mime_type.startswith("audio/"):
            audio_file = msg.document
        else:
            await msg.reply_text(MESSAGES["not_audio"])
            return
    else:
        await msg.reply_text(MESSAGES["not_audio"])
        return
    
    # File size check
    file_size = getattr(audio_file, 'file_size', 0)
    if file_size and file_size > MAX_FILE_SIZE:
        logger.warning(f"User {user_id}: file too large ({file_size} bytes)")
        await msg.reply_text(MESSAGES["file_too_large"])
        return
    
    try:
        file = await context.bot.get_file(audio_file.file_id)
        
        if file.file_size and file.file_size > MAX_FILE_SIZE:
            await msg.reply_text(MESSAGES["file_too_large"])
            return
        
        audio_bytes = await file.download_as_bytearray()
        
        # Determine mime type
        if file_type == "voice":
            mime_type = "audio/ogg"
        elif hasattr(audio_file, 'mime_type') and audio_file.mime_type:
            mime_type = audio_file.mime_type
        else:
            mime_type = "audio/mpeg"
        
        # Cache audio
        user_audio_cache[user_id] = {
            "data": bytes(audio_bytes),
            "mime_type": mime_type
        }
        
        logger.info(f"‚úÖ Audio cached: user={user_id}, size={len(audio_bytes)}, mime={mime_type}")
        await msg.reply_text(MESSAGES["audio_received"], reply_markup=get_menu_keyboard())
        
    except Exception as e:
        logger.error(f"Error downloading audio for user {user_id}: {e}")
        await msg.reply_text(MESSAGES["error"])


async def process_with_cascade(
    audio_data: bytes,
    mime_type: str,
    mode: str
) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    Process audio with model cascade.
    Returns: (result_text, model_used, error_message)
    """
    audio_b64 = base64.standard_b64encode(audio_data).decode("utf-8")
    last_error = None
    quota_exhausted = False
    
    prompt = PROMPTS.get(mode, PROMPTS["summary"])
    
    for i, model_name in enumerate(MODEL_PRIORITY):
        try:
            logger.info(f"üîÑ Trying model {i+1}/{len(MODEL_PRIORITY)}: {model_name}")
            
            model = genai.GenerativeModel(model_name)
            
            # Create content with audio inline
            response = await asyncio.to_thread(
                model.generate_content,
                [
                    {"inline_data": {"mime_type": mime_type, "data": audio_b64}},
                    prompt
                ],
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": 8192
                }
            )
            
            if response.text:
                logger.info(f"‚úÖ Success with: {model_name}")
                return response.text, model_name, None
            else:
                logger.warning(f"‚ö†Ô∏è Empty response from {model_name}")
                last_error = "Ÿæÿßÿ≥ÿÆ ÿÆÿßŸÑ€å ÿßÿ≤ ŸÖÿØŸÑ"
                continue
                
        except google_exceptions.NotFound as e:
            logger.warning(f"‚ùå {model_name} - Not found: {str(e)[:50]}")
            last_error = f"ŸÖÿØŸÑ {model_name} €åÿßŸÅÿ™ ŸÜÿ¥ÿØ"
            continue
            
        except google_exceptions.ResourceExhausted as e:
            logger.warning(f"‚ùå {model_name} - Quota exhausted")
            quota_exhausted = True
            last_error = "ÿ≥ŸÇŸÅ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿ±ÿß€å⁄ØÿßŸÜ API ÿ™ŸÖÿßŸÖ ÿ¥ÿØŸá"
            continue
            
        except google_exceptions.InvalidArgument as e:
            error_str = str(e)
            logger.warning(f"‚ùå {model_name} - Invalid: {error_str[:80]}")
            if "audio" in error_str.lower():
                last_error = "ÿß€åŸÜ ŸÖÿØŸÑ ÿßÿ≤ ÿµÿØÿß Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÜŸÖ€å‚Äå⁄©ŸÜÿØ"
            else:
                last_error = f"Ÿæÿßÿ±ÿßŸÖÿ™ÿ± ŸÜÿßŸÖÿπÿ™ÿ®ÿ±: {error_str[:50]}"
            continue
            
        except google_exceptions.PermissionDenied as e:
            logger.error(f"‚ùå {model_name} - Permission denied")
            last_error = "API Key ŸÖÿπÿ™ÿ®ÿ± ŸÜ€åÿ≥ÿ™ €åÿß ÿØÿ≥ÿ™ÿ±ÿ≥€å ŸÜÿØÿßÿ±€åÿØ"
            continue
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå {model_name} - Error: {error_msg[:100]}")
            last_error = error_msg[:80]
            continue
    
    # Determine final error message
    if quota_exhausted:
        final_error = "‚ö†Ô∏è ÿ≥ŸÇŸÅ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿ±ÿß€å⁄ØÿßŸÜ API ÿ™ŸÖÿßŸÖ ÿ¥ÿØŸá!\n\nüí° ŸÑÿ∑ŸÅÿßŸã:\n‚Ä¢ ⁄ÜŸÜÿØ ÿØŸÇ€åŸÇŸá ÿµÿ®ÿ± ⁄©ŸÜ€åÿØ\n‚Ä¢ €åÿß API Key ÿ¨ÿØ€åÿØ ÿ®⁄Ø€åÿ±€åÿØ"
    else:
        final_error = last_error or "ÿÆÿ∑ÿß€å ŸÜÿßÿ¥ŸÜÿßÿÆÿ™Ÿá"
    
    logger.error(f"‚ùå All models failed! Last error: {last_error}")
    return None, None, final_error


async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle button callbacks."""
    query = update.callback_query
    await query.answer()
    
    user_id = update.effective_user.id
    mode = query.data
    
    if user_id not in user_audio_cache:
        await query.edit_message_text(MESSAGES["no_audio"])
        return
    
    try:
        audio_info = user_audio_cache[user_id]
        
        mode_names = {
            "lecture": "üìö ÿØÿ±ÿ≥ŸÜÿßŸÖŸá ⁄©ÿßŸÖŸÑ",
            "soap": "ü©∫ ÿ¥ÿ±ÿ≠‚Äåÿ≠ÿßŸÑ Ÿæÿ≤ÿ¥⁄©€å",
            "summary": "üìù ÿÆŸÑÿßÿµŸá ŸÖÿ™ŸÜ",
            "lyrics": "üéµ ŸÖÿ™ŸÜ ÿ¢ŸáŸÜ⁄Ø"
        }
        
        await query.edit_message_text(
            f"{MESSAGES['processing']}\n\nüéØ ÿ≠ÿßŸÑÿ™: {mode_names.get(mode, mode)}"
        )
        
        result, model_used, error = await process_with_cascade(
            audio_info["data"],
            audio_info["mime_type"],
            mode
        )
        
        if result:
            # Success
            header = f"‚úÖ **{mode_names.get(mode, 'Ÿæÿ±ÿØÿßÿ≤ÿ¥')} ⁄©ÿßŸÖŸÑ ÿ¥ÿØ**\n\n"
            footer = f"\n\n---\nü§ñ ŸÖÿØŸÑ: `{model_used}`"
            full_text = header + result + footer
            
            # Handle long messages
            if len(full_text) > 4000:
                try:
                    await query.edit_message_text(full_text[:4000], parse_mode="Markdown")
                except Exception:
                    await query.edit_message_text(full_text[:4000])
                
                remaining = full_text[4000:]
                while remaining:
                    chunk = remaining[:4000]
                    remaining = remaining[4000:]
                    await asyncio.sleep(0.5)  # Rate limit protection
                    try:
                        await context.bot.send_message(
                            chat_id=update.effective_chat.id,
                            text=chunk,
                            parse_mode="Markdown"
                        )
                    except Exception:
                        await context.bot.send_message(
                            chat_id=update.effective_chat.id,
                            text=chunk
                        )
            else:
                try:
                    await query.edit_message_text(full_text, parse_mode="Markdown")
                except Exception:
                    await query.edit_message_text(full_text)
        else:
            # Failed - show detailed error
            await query.edit_message_text(f"‚ùå {error}")
    
    except Exception as e:
        logger.error(f"Callback error for user {user_id}: {e}")
        logger.error(traceback.format_exc())
        try:
            await query.edit_message_text(f"‚ùå ÿÆÿ∑ÿß: {str(e)[:100]}")
        except Exception:
            pass
    
    finally:
        # Always cleanup cache
        if user_id in user_audio_cache:
            del user_audio_cache[user_id]
            logger.info(f"üßπ Cache cleaned: user={user_id}")


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle errors."""
    logger.error(f"Error: {context.error}")
    if update:
        logger.error(f"Update: {update}")


def main() -> None:
    """Start the bot."""
    print("\n" + "="*60)
    print("  üéß OMNI-HEAR AI v2.4 - Fixed Model Names")
    print("="*60)
    
    # Validate tokens
    if not TELEGRAM_BOT_TOKEN:
        logger.error("‚ùå TELEGRAM_BOT_TOKEN not set!")
        print("\n‚ö†Ô∏è  Set: TELEGRAM_BOT_TOKEN=your_token")
        sys.exit(1)
    
    if not GEMINI_API_KEY:
        logger.error("‚ùå GEMINI_API_KEY not set!")
        print("\n‚ö†Ô∏è  Set: GEMINI_API_KEY=your_key")
        print("   Get it from: https://aistudio.google.com/app/apikey")
        sys.exit(1)
    
    print(f"‚úÖ Telegram: Connected")
    print(f"‚úÖ Gemini: Configured")
    print(f"üîÑ Models: {' ‚Üí '.join(MODEL_PRIORITY)}")
    print("="*60 + "\n")
    
    # Build application
    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    
    # Add handlers
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("status", status_command))
    app.add_handler(CommandHandler("models", models_command))
    app.add_handler(MessageHandler(
        filters.VOICE | filters.AUDIO | filters.Document.AUDIO,
        handle_audio
    ))
    app.add_handler(CallbackQueryHandler(button_callback))
    app.add_error_handler(error_handler)
    
    # Run
    logger.info("üöÄ Bot starting...")
    app.run_polling(allowed_updates=Update.ALL_TYPES, drop_pending_updates=True)


if __name__ == "__main__":
    main()
